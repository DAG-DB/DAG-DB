2022-09-10 BST 17:26:31,529 - INFO - 
Log for run 2022-09-10_BST_17:26:31,524_sachs_dag_training_lambda_1

2022-09-10 BST 17:26:31,529 - INFO - COMMENT = 'IMLE_Logistic_None for Sachs data with DAG traning'
2022-09-10 BST 17:26:31,529 - INFO - DATA_FOLDER = '../data'
2022-09-10 BST 17:26:31,529 - INFO - LATENTS_FOLDER = '../latents'
2022-09-10 BST 17:26:31,529 - INFO - LOGS_FOLDER = '../logs'
2022-09-10 BST 17:26:31,530 - INFO - MODEL_FOLDER = '../models'
2022-09-10 BST 17:26:31,530 - INFO - RESULTS_FOLDER = '../results'
2022-09-10 BST 17:26:31,530 - INFO - DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Used: "cuda"
2022-09-10 BST 17:26:31,530 - INFO - LATENTS = False
2022-09-10 BST 17:26:31,530 - INFO - MATRIX_REPORT = False
2022-09-10 BST 17:26:31,530 - INFO - LABEL = True
2022-09-10 BST 17:26:31,530 - INFO - SAVE_TO_ODS = False
2022-09-10 BST 17:26:31,530 - INFO - VAL_LOSS_FROM_DAG = True
2022-09-10 BST 17:26:31,530 - INFO - WANDB = False
2022-09-10 BST 17:26:31,530 - INFO - WANDB_PROJECT = 'Thetas'
2022-09-10 BST 17:26:31,530 - INFO - GRAD_LOG = False
2022-09-10 BST 17:26:31,530 - INFO - SAVE_RESULTS = True
2022-09-10 BST 17:26:31,530 - INFO - N_TRIALS = 1
2022-09-10 BST 17:26:31,530 - INFO - LOAD_OPTUNA_STUDY = None
2022-09-10 BST 17:26:31,530 - INFO - OPTUNA_STRINGS = {'None': None, 'True': True, 'False': False}
2022-09-10 BST 17:26:31,530 - INFO - TQDM = False
2022-09-10 BST 17:26:31,530 - INFO - 
hyperparameters = {
2022-09-10 BST 17:26:31,530 - INFO - 	'GRAPH_TYPE': None,
2022-09-10 BST 17:26:31,530 - INFO - 	'D': 11,
2022-09-10 BST 17:26:31,530 - INFO - 	'MAX_SIZE': None,
2022-09-10 BST 17:26:31,530 - INFO - 	'SEM_NOISE': 'gaussian_ev',
2022-09-10 BST 17:26:31,530 - INFO - 	'VAL': 0.009378663540445486,
2022-09-10 BST 17:26:31,530 - INFO - 	'N_EPOCHS': 1000,
2022-09-10 BST 17:26:31,530 - INFO - 	'BATCH_SIZE': None,
2022-09-10 BST 17:26:31,530 - INFO - 	'LOG2_BATCH_SIZE': 3,
2022-09-10 BST 17:26:31,530 - INFO - 	'BATCH_SHUFFLE': True,
2022-09-10 BST 17:26:31,530 - INFO - 	'N_SAMPLES': 47,
2022-09-10 BST 17:26:31,530 - INFO - 	'MODE': 'max_dag',
2022-09-10 BST 17:26:31,530 - INFO - 	'AFAS_SCHEDULER': None,
2022-09-10 BST 17:26:31,530 - INFO - 	'MINIZINC_SOLVER': None,
2022-09-10 BST 17:26:31,530 - INFO - 	'f_LINEAR_LAYERS': (),
2022-09-10 BST 17:26:31,530 - INFO - 	'f_BIAS': False,
2022-09-10 BST 17:26:31,530 - INFO - 	'p_LAYER': None,
2022-09-10 BST 17:26:31,530 - INFO - 	'STREAMLINE': False,
2022-09-10 BST 17:26:31,530 - INFO - 	'THRESHOLD': 0.0,
2022-09-10 BST 17:26:31,530 - INFO - 	'h_LINEAR_LAYERS': (),
2022-09-10 BST 17:26:31,530 - INFO - 	'h_NULL': 0.00011373901985660933,
2022-09-10 BST 17:26:31,530 - INFO - 	'h_LR': 0.0016158450337500668,
2022-09-10 BST 17:26:31,530 - INFO - 	'h_OPTIMIZER': 'torch.optim.Adam(overall_net.h_net.parameters(), lr=h.h_LR)',
2022-09-10 BST 17:26:31,530 - INFO - 	'f_LR': 0.3720066380139224,
2022-09-10 BST 17:26:31,530 - INFO - 	'f_OPTIMIZER': 'torch.optim.Adam(overall_net.f_net.parameters(), lr=h.f_LR)',
2022-09-10 BST 17:26:31,530 - INFO - 	'NOISE_TEMPERATURE': 0.8785929148606203,
2022-09-10 BST 17:26:31,530 - INFO - 	'NOISE_DISTRIBUTION': 'LogisticNoiseDistribution()',
2022-09-10 BST 17:26:31,531 - INFO - 	'LAMBDA': 1,
2022-09-10 BST 17:26:31,531 - INFO - 	'LOSS_FN': 'torch.nn.MSELoss()',
2022-09-10 BST 17:26:31,531 - INFO - 	'ALPHA': None,
2022-09-10 BST 17:26:31,531 - INFO - 	'BETA': None,
2022-09-10 BST 17:26:31,531 - INFO - 	'REGULARIZER': None,
2022-09-10 BST 17:26:31,531 - INFO - 	'z_RHO': 0.157451211201,
2022-09-10 BST 17:26:31,531 - INFO - 	'z_MU': 0.00120806965197976,
2022-09-10 BST 17:26:31,531 - INFO - 	'z_REGULARIZER': 'NoTearsZRegularizer(h.D, h.z_RHO, h.z_MU, c.DEVICE)',
2022-09-10 BST 17:26:31,531 - INFO - 	'LR_SCHEDULER': None,
2022-09-10 BST 17:26:31,531 - INFO - 	'DATA_CATEGORY': 'real',
2022-09-10 BST 17:26:31,531 - INFO - 	'DAGS': 'Sachs_1',
2022-09-10 BST 17:26:31,531 - INFO - 	'N_REAL_RUNS': 1
2022-09-10 BST 17:26:31,531 - INFO - }

2022-09-10 BST 17:26:31,808 - INFO - 

DAG 0: size_true=17	X.var()=43694.6783
2022-09-10 BST 17:26:44,471 - INFO -  epoch=   1      train_loss=29089.5174      val_loss=26641.3223      best=1      size=  24      nSHD_c=   2.4545      nSHD=   2.2727      prec_c=   0.0833      change_adj_count=0      
2022-09-10 BST 17:27:02,914 - INFO -  epoch=   3      train_loss=44789.5336      val_loss=24201.1426      best=1      size=  15      nSHD_c=   1.6364      nSHD=   1.6364      prec_c=   0.2667      change_adj_count=2      
2022-09-10 BST 17:27:20,661 - INFO -  epoch=   5      train_loss=50647.7530      val_loss=18046.6680      best=1      size=  10      nSHD_c=   1.3636      nSHD=   1.3636      prec_c=   0.4000      change_adj_count=4      
2022-09-10 BST 17:29:14,920 - INFO -  epoch=  18      train_loss=32031.6090      val_loss=17654.5332      best=1      size=  10      nSHD_c=   1.2727      nSHD=   1.3636      prec_c=   0.4000      change_adj_count=14      
2022-09-10 BST 17:31:22,015 - INFO -  epoch=  33      train_loss=43060.4908      val_loss=14933.8047      best=1      size=   9      nSHD_c=   1.4545      nSHD=   1.5455      prec_c=   0.3333      change_adj_count=22      
2022-09-10 BST 17:40:31,625 - INFO -  epoch= 100      train_loss=27721.4614      val_loss=25636.5781      best=0      size=   6      nSHD_c=   1.1818      nSHD=   1.2727      prec_c=   0.6667      change_adj_count=62      
2022-09-10 BST 17:53:54,883 - INFO -  epoch= 200      train_loss=33915.2679      val_loss=26271.8730      best=0      size=   5      nSHD_c=   1.2727      nSHD=   1.3636      prec_c=   0.6000      change_adj_count=113      
2022-09-10 BST 18:07:15,408 - INFO -  epoch= 300      train_loss=37507.1953      val_loss=40743.9062      best=0      size=   7      nSHD_c=   1.3636      nSHD=   1.5455      prec_c=   0.4286      change_adj_count=157      
2022-09-10 BST 18:20:31,898 - INFO -  epoch= 400      train_loss=73715.4147      val_loss=12745.0029      best=1      size=   7      nSHD_c=   1.1818      nSHD=   1.2727      prec_c=   0.5714      change_adj_count=217      
2022-09-10 BST 18:33:58,108 - INFO -  epoch= 500      train_loss=39630.6863      val_loss=27571.3184      best=0      size=   6      nSHD_c=   1.0000      nSHD=   1.3636      prec_c=   1.0000      change_adj_count=267      
2022-09-10 BST 18:47:26,304 - INFO -  epoch= 600      train_loss=32688.7105      val_loss=24990.1250      best=0      size=   4      nSHD_c=   1.1818      nSHD=   1.3636      prec_c=   1.0000      change_adj_count=305      
2022-09-10 BST 18:52:25,207 - INFO -  epoch= 637      train_loss=40137.1710      val_loss=12195.7842      best=1      size=   6      nSHD_c=   1.2727      nSHD=   1.3636      prec_c=   0.5000      change_adj_count=324      
2022-09-10 BST 19:00:54,405 - INFO -  epoch= 700      train_loss=22920.5844      val_loss=22285.3613      best=0      size=   6      nSHD_c=   1.3636      nSHD=   1.4545      prec_c=   0.5000      change_adj_count=356      
2022-09-10 BST 19:14:15,902 - INFO -  epoch= 800      train_loss=21237.2583      val_loss=21895.1152      best=0      size=   6      nSHD_c=   1.2727      nSHD=   1.3636      prec_c=   0.5000      change_adj_count=407      
2022-09-10 BST 19:27:45,143 - INFO -  epoch= 900      train_loss=32185.2381      val_loss=27953.1992      best=0      size=   7      nSHD_c=   1.1818      nSHD=   1.3636      prec_c=   0.5714      change_adj_count=464      
2022-09-10 BST 19:35:51,336 - INFO -  epoch= 960      train_loss=44589.2203      val_loss=11578.3984      best=1      size=   7      nSHD_c=   1.1818      nSHD=   1.2727      prec_c=   0.5714      change_adj_count=502      
2022-09-10 BST 19:41:13,257 - INFO -  epoch=1000      train_loss=21062.5493      val_loss=26447.3066      best=0      size=   6      nSHD_c=   1.2727      nSHD=   1.3636      prec_c=   0.5000      change_adj_count=524      
2022-09-10 BST 19:41:13,260 - INFO - Last epoch results saved in sachs_dag_training_lambda_1.csv
2022-09-10 BST 19:41:13,260 - INFO - 
last_mean_nshd_c=1.2727
2022-09-10 BST 19:41:13,260 - INFO - 
Time taken 2:14:42 (h:mm:ss)
2022-09-10 BST 19:41:13,260 - INFO - 
End of log for run 2022-09-10_BST_17:26:31,524_sachs_dag_training_lambda_1
